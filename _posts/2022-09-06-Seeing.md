---
title: Seeing
tags: AI Neurology Art
article_header:
  type: cover
  image:
    src: /screenshot.jpg
---
TEST
A Post with Header Image, See [Page layout](https://tianqi.name/jekyll-TeXt-theme/samples.html#page-layout) for more examples.
jghkhvjhvjhvjh


## Intro
  there’s some amazing work being done with AI, and with scale more and more problems appear to be solve-able. Here’s what some of that looks like
There’s a back and forth in AI, about what the trajectory of current progress looks like. You have people (who?) saying we’re in an AI winter, and a winter where we’ve picked all the low hanging fruit (chollet, lex friedman?). We have another camp throwing more and more compute at the problem enabled by architectures like attention and the availability of ever more data (and methods for handling that data (leave this in) ). This approach has led to astounding results in a variety of hard problems. We have GATO from deepmind applying a large transformer scaling approach over it’s more traditional RL. OpenAI perhaps represents the very best of this approach, and many of the claims about scaling laws originate from this world class lab(check, provide source). We have incredibly impressive large language models like GPT3, their use in what is effectively program synthesis from natural language with CODEX (and available as github co-pilot), amazing classification performance and utility of CLIP, which leads us to the news now. 

DALLE-2. It is what made this. This is beautiful. 
<blockquote class="twitter-tweet"><p lang="en" dir="ltr">This was made using the DALLE-2 neural network to extend Michaelangelo&#39;s creation of Adam. <a href="https://t.co/sIfTEjXgwO">pic.twitter.com/sIfTEjXgwO</a></p>&mdash; Far Left Kyle (@FLKDayton) <a href="https://twitter.com/FLKDayton/status/1543261364315193346?ref_src=twsrc%5Etfw">July 2, 2022</a></blockquote> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script> 
<!--add more to really get across the point--> 
This is also Dalle. 
<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Some people sell AI for grading essays and sorting resumes. Imagine how wrong it must quietly, constantly, be. <a href="https://t.co/KRuaacYHAo">https://t.co/KRuaacYHAo</a></p>&mdash; Janelle Shane (@JanelleCShane) <a href="https://twitter.com/JanelleCShane/status/1526225473272938501?ref_src=twsrc%5Etfw">May 16, 2022</a></blockquote> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script> 

## Quietly Wrong
Janelle Shane runs an excellent blog I was recently introduced to, collecting a variety of examples of… well, weird AI. This ‘weirdness’ is well known and taken at varying levels of seriousness by the many practitioners, researchers, and sellers (change to providers?) of AI systems. The grab bag of AI systems are vulnerable to the grab bag of biases and weirdness. After all, if there is a flawed but useful pattern, an un-truth, it can (and most often will) be picked up on. 

| ![Suprious Correlations]( {{ "/assets/images/logo/chartsoy.png" | relative_url }} ) |
|:--:|
| *Perhaps we should stop subsidizing soy? Afterall it's 99% correlated* [![Spurious Correlations](https://www.tylervigen.com/spurious-correlations)](https://www.tylervigen.com/spurious-correlations) |

The untruth could be a curiosity like that. It could recommend the right course of action, if for the wrong reasons. Or it could dig up the corpse of Jim Crow, puppeting its bones. Of particular importance are deep learning systems not just for their ‘quietness’ but very real power, and resulting popularity. Maybe even more than that, is their fascinating weirdness. 

Now, I can’t say whether I’m hot or cold on deep learning. These systems will absolutely have a role in humanities’ future, and should. They can do such amazing things. More than that, they are beautiful. What can be made is visually striking and regardless of whether you’d deem it art or not it is visually striking. 
Dalle2 is not particularly special, we have Dalle 1 (which under the hood is very different model


| ![Dalle 1 Cats](/assets/images/first_blog/Dalle1.png) |
|:--:|
| *Dalle 1 producing weird Cats* [![Open AI's Dalle 1](https://openai.com/blog/dall-e/)](https://openai.com/blog/dall-e/)  |

| ![Dalle 1 Cats](/assets/images/first_blog/disco_diff.webp) |
|:--:|
| *A Disco Diffusion image from reddit user komoro* [![Reddit page](https://www.reddit.com/r/DiscoDiffusion/comments/vyeumc/im_not_entirely_sure_why_it_escalated_into_this/)](https://www.reddit.com/r/DiscoDiffusion/comments/vyeumc/im_not_entirely_sure_why_it_escalated_into_this/)  |

| ![Wombo AI](/assets/images/first_blog/wombo.png){: width="250" } |
|:--:|
| *Wombo AI which at the time used Open AI's CLIP and VQ-GAN* [![ try Wombo AI](https://www.wombo.art)](https://www.wombo.art/)  |

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Caucasian man with somewhat exaggerated features looking sceptically at the viewer, <a href="https://twitter.com/hashtag/art?src=hash&amp;ref_src=twsrc%5Etfw">#art</a> made with <a href="https://twitter.com/hashtag/Artbreeder?src=hash&amp;ref_src=twsrc%5Etfw">#Artbreeder</a>.<a href="https://t.co/3fs8suGMsX">https://t.co/3fs8suGMsX</a> <a href="https://t.co/mxlhHNprnf">pic.twitter.com/mxlhHNprnf</a></p>&mdash; Gniwu (@gniwukiwi) <a href="https://twitter.com/gniwukiwi/status/1545474647713611781?ref_src=twsrc%5Etfw">July 8, 2022</a></blockquote> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script> 

Last is ArtBreeder (Based on PicBreeder and NEAT by the excellent AI researcher Kennth Stannely ). Now compare ArtBreeder, which is much much older (at least in terms of AI technology) but still gets pretty damn good results. Yes, these are cherry picked but maybe even more than you may realize. You see, Artbreeder uses large groups of humans (the users) to make decisions about what kinds of images to produce in a feedback loop determining which images are slightly morphed and presented next round. 

Naively you might expect humans to randomly select paths
through this abstract image space. Less naively, you might expect the search behavior to aggregate around some kind of average behavior.
But you see, what we see is so totally different from what’s in front of you. People see faces, dragons, and telephones in clouds. They don’t see vaporized water molecules. They don’t see atoms. They don’t see climate patterns. We perceive at a certain scale and even more we filter out what we do not prefer, seek what we do, ignore what does not align with our goals, and force it into our internal narrative.
Flawed process that it is, it pulls this 

| ![Art Breeder](/assets/images/first_blog/artbredland.png) |
| *A  "bred" series of landscapes* [![ try ArtBreeder](https://www.artbreeder.com/beta/image/ca8a30f25395cceb7e930da01aa2)](https://www.artbreeder.com/beta/image/ca8a30f25395cceb7e930da01aa2)  |

from this

| ![BigGan](/assets/images/first_blog/bigganland.png) |
|:--:|
| *One Of ArtBreeder's Underlying Models BigGAN* [![ BigGAN and Weird AI](https://www.aiweirdness.com/imaginary-worlds-dreamed-by-biggan-18-09-30/)](https://www.aiweirdness.com/imaginary-worlds-dreamed-by-biggan-18-09-30/)  |

If you don’t believe me, I can’t recommend playing around with it yourself enough. With your strange, flawed, and weird way of looking at the world, you can see stuff. Whole and coherent objects from the cacophonous orchestra of all reality, all the time, at all scales things appear before you with as little effort as you use to breathe.
Whereas an AI can see the world like this: 

| ![BigGan](/assets/images/first_blog/biganWrong.png) |
|:--:|
| *A BigGAN image, can you see anything here?* [![ BigGAN and Weird AI](https://www.aiweirdness.com/imaginary-worlds-dreamed-by-biggan-18-09-30/)](https://www.aiweirdness.com/imaginary-worlds-dreamed-by-biggan-18-09-30/)  |

I  would recommend spending some more time looking at the previous images. If you look closely at details like faces, boundaries, foreground-background distinction, you’ll notice things are just slightly wrong (sometimes more than slightly), sometimes obfuscated as paint strokes or other artistic flourishes. In a variety of models, made in a variety of different ways, with different generation seeds, with different purposes I think you will notice a shadow of the same wrongness apparent in this image.

Now, I did somewhat mislead you earlier. In fact, only part of you sees stuff, and other parts see other (equally important) aspects of reality. Looking into these parts can help us start  to understand this big mess, and maybe how to start getting us out of it. 

## System 1 and System 2
Now I have a little riddle for you.
If you buy a ball and a bat from a sports store for $1.10, and the bat costs one dollar more than the ball, how much did the ball cost? 

Most people answer 10 cents. And of course if the bat were to cost a dollar more, it would cost $1.10. So together they would cost $1.20 which unfortunately is not $1.10. 

| ![A Tragedy](/assets/images/first_blog/base.png){width= "250"} |
|:--:|
| *I know I have no shame.*   |

[![Youre welcome to try another riddle](https://www.considerable.com/life/education/difficult-straightforward-word-problem/)](https://www.considerable.com/life/education/difficult-straightforward-word-problem/)

Nobel prize winning economist Danial Kahnmann attributes this mistake to something called system 1, and conversely if you answered 5 cents to system 2. This dichotomy is an abstraction placed on top of the brain, not pointing to any specific function or part, which describes overall modes of cognition. Fundamentally system 1 is fast, intuitive, automatic, and has a broad attention(find other word). In contrast, system 2 is slow, deliberate, and has a specific attention. Kahnmann identified this distinction as part of his larger research dealing with human decision making, that is rationality (and often irrationality), in economics. His contributions include coining the term cognitive bias. 

It would be a mistake to say that we should aspire to exclusively interact with the world using our type 2 system. Both are necessary, and what we see as grace when a tennis player moves to fluidly hit the ball, or a painter's rapid rendering of a form is mediated by system 1. It’s about using  each where they belong, and so while we may need more of system 2, it’s not as though we could do without system 1 altogether. 

For some time AI researchers (like joshua bengo) have identified with the system 1 vs system 2 dichotomy, noting the need for the more meta-cognizant, exact, and deliberate (if slow) characteristic of system 2 to complement the fluid and opaque decisions being made by current technologies.

Somewhat recently Meta (then facebook) AI labs presented a model capable of solving many of the same problems as computer algebra systems like wolfram mathematica, maple, and matlab (check this one). Algebra is very much a system 2 task, requiring slow and methodological action. Manipulation of symbols is often described as a system 2 task. The AI operates on the symbolic equation (as opposed to numerically solving it), using similar methods to translation models. Unlike you or me, it learns statistical patterns from large sets of equation-solution pairs. This makes it faster than normal computer algebra systems, but also allows it to have learned approximate ( and thus potentially wrong, which it did) patterns. Normal computer algebra systems have no such issue. 

It can be easy to mistake a symbol for the thing itself. We need to keep in mind that it represents *something*. It has its own rules, and unique properties that a symbol can obfuscate. When we work at the symbolic level, and only the symbolic level,[![We can easily prove 1=2](https://www.youtube.com/watch?v=hI9CaQD7P6I)](https://www.youtube.com/watch?v=hI9CaQD7P6I). Symbols are not enough.  

Given a photo of a canine, you could tell if it was a wolf or a dog in the image. Maybe you’d decide based on snout shape, or fur color. You’ll likely have a list of criteria along with a sort of “I know it when I see it” intuition. The former we can associate with system 2 and the latter with system 1. We can train a model to distinguish dogs from wolves as well. It can achieve greater than 90% accuracy. It could outperform most humans. But what it can learn is that a white patch in the image means it should categorize it as a wolf. I need to say, it’s not the presence of snow, but simply the prevalence of white pixels irrespective of whatever other features exist in the image. It’s not learning a correlation so much as an incorrect rule. A fully white image is not a picture of a wolf. But to this model, it is, because most of the wolf images it saw had snow and thus a prevalence of white pixels, because of how humans tend to photograph wolves. This is the short-cut principle (cite): a model will learn the quickest, least expensive rule that leads to the best accuracy given the data it has seen. 

Maybe you don’t care, getting a wolf vs a dog wrong won’t change your life. However these models are being deployed right now. They’re used to determine loan approval. They were already used to determine parole. They’re proposed to detect cancer in x-rays, and grade student essays. These flaws make them vulnerable to attackers, who can discover and then exploit these short-cut rules and oddities.

While the goal of mimicking the system 1 and 2 dichotomy will likely be helpful in further advancing deep learning, it alone won’t be enough. Contextual awareness is deeply lacking in deep learning models, as is objectness bias (that is the tendency to organize the world into objects, instead of say atoms) which are clearly unconscious system 1 behaviors. Further system 2’s attention is focused, deliberate, and singular. It does not allow for the broad, and open attention needed for many tasks or fully understanding context. Worse yet, the symbol focused rationalistic system 2 can find itself losing what the symbol actually represents. Much of what we want from deep learning models is the automation of behaviors attributable to system 1, like driving. 

While the interplay between system 1 and 2 is desirable, with system 2 working as a teacher and exception handler for system 1, many of the issues with AI we currently have won’t be solved by it because many of the issues we have with deep learning require not just a better system 2 (deliberate) but also a better  system 1 (intuitive). It lacks the ability to properly explain many of the issues we currently see in AI, and even carries potential harm in learning the wrong symbolic logic (the trite example where an AI learns to maximize human happiness by killing us all). It is a useful abstraction, but it is simply not enough.

An appealing alternative can be found in the more neurologically grounded right-left brain dichotomy.  Unfortunately there are many misconceptions of left-right brain dichotomy as it has leaked into pop-science. Let’s cover these now and clear things up. Because of this misconception the left brain is considered to be somewhat cold, logical, and rational whereas the right is seen as creative, emotional, and non-linguistic. This is not precisely untrue, but leads to incorrect conclusions. While broadly the left hemisphere deals with the grammar and mechanics of language, the right is used to understand the meaning and context of language. The ability to understand metaphors, thought to be fundamental to human cognition and often deeply lacking in deep learning models, is a right hemisphere task. Both hemispheres play a role in tasks like mathematics, where the left manipulates symbols and right searches for understanding of the underlying structure. People (and for that matter animals) use both hemispheres, for their respective strengths, to accomplish many goals. There is no being left or right brained.  So far, it might be easy to identify the left hemisphere with system 2 and the right with system 1. 


<!--more-->
